% TODO: Bibliografia
\chapter{Estado del Arte}\label{chapter:state-of-the-art}

\section{Optimizaci\'on Multiobjetivo}

Optimizaci\'on Multiobjetivo es la rama de la optimizaci\'on que se dedica a optimizar varias funciones sim\'ultaneamente.\\

\textit{Optimizaci\'on Multiobjetivo}: Dado $m$ funciones objetivos: $f_1: \mathcal{X} \rightarrow \mathbb{R}, ..., f_m: \mathcal{X} \rightarrow \mathbb{R}$ que dado un vector en el espacio de decisi\'on $\mathcal{X}$  $\mathbb{R}$. Un problema multiobjetivo esta dado por:
\begin{align*}
    \min f_1(x), ..., f_m(x), x \in \mathcal{X}
\end{align*}

En la presencia de varias funciones objetivos ya no es posible hablar de soluci\'on \'unica o m\'inimo. Es usual que estos criterios de decisi\'on entren en conflictos entre s\'i y no sea posible encontrar una soluci\'on que satisfaga ambas m\'etricas, en cambio, se encuentran soluciones que si se intentan optimizar en algun aspecto, inevitablemente empeoran en otros. Estas soluciones son las que se conocen como soluciones del frente de Pareto. Con el concepto de Pareto viene el concepto de Pareto dominaci\'on.\\

\textit{Pareto Dominaci\'on}: Dados vectors en el el espacio objetivo, $x \in \mathbb{R}^m$ y $y \in \mathbb{R}^m$ , se dice que $x$ Pareto domina a $y$ (i.e. $x \prec y$), si y solo si:
\begin{align*}
    \forall i \in \{1, ..., m\}: x_i \leq y_i \text{ and } \exists j \in \{1, ..., m\}: x_j < y_j 
\end{align*}

\textit{Frente de Pareto}: El conjunto de soluciones que no son dominadas por ningun otra soluci\'on en el espacio.\\

Encontrar soluciones en el frente de Pareto no es tarea d\'ificil, basta con encontrar el m\'inimo de cada funci\'on objetivo, soluciones que indudablemente partenecen al frente de Pareto. La tarea d\'ificil se encuentra en tratar una aproximaci\'on del frente de Pareto que sea representativa de este.

Se debe notar que el termino Optimizaci\'on Multiobjetivo suele referirse cuando se intenta optimzar simult\'aneamente para dos o tres funciones objetivos. Para un cantidad de criterios de optimizaci\'on grande se le suele conocer coloquialmente Optimizaci\'on para Muchos Objetivos o \textit{many-objective optimization} en ingl\'es. Existe una diferenciaci\'on en este pues al aumentar le n\'umero de m\'etricas a optimizar:
\begin{enumerate}
    \item ya no es posible la visualizaci\'on del frente;
    \item la computaci\'on de indicadores o de selecci\'on se convierte en problemas NP-duros;
    \item existe un r\'apido crecimiento de puntos no dominados, mientras mayor n\'umero de objetivos, la probabilidad de que un punto sea no dominado en un set con distribuci\'on normal tiende exponencialmente a 1.
\end{enumerate}


\subsection{T\'ecnicas de Escalarizaci\'on (Scalarization)}
Los primeros intentos de resolver el MOP fue utilizando t\'ecinas de Escalarizaci\'on, que en resumen es cuando de alguna manera u otra se combinan todas las funciones objetivos en una sola. Existen varias maneras de esto:
\begin{enumerate}
    \item Linear Weighting: Todas las funciones objetivos se combinan en una sola, y a cada una se le asigna un peso. Modificando estos pesos se obtiene una soluci\'on distinta del frente de Pareto.\\
    \begin{align*}
        \min \sum w_i f_i(x), \space x \in X
    \end{align*}
    Esta enfoque presenta el problema de que cuando el frente de Pareto no es convexo (tiene alguna porci\'on c\'oncava) no es posible obtener soluciones en esta zona, no importa como se modifiquen los pesos $w_i$

    \item $\epsilon$-constrain: Se selecciona una funci\'on objetivo como principal y las dem\'as se establecen como restricciones de esta, y tienen que ser menor que sierto $\epsilon_i$ con $1 \leq n - 1$ por cada funci\'on objetivo.
    \begin{align*}
            \min  f_1(x), \space x \in X  & \text{, sujeto a:}   \\
            g_i(x) \leq \epsilon_i & \quad  \forall i, 0 \leq i \leq n - 1
    \end{align*}
    Utilizar esta t\'ecninca tiene dos dificultades principales, primero la selecci\'on de los $\epsilon_i$, que para una adecuada selecci\'on requiren conocimiento previo del frente de Pareto y al igual que \textit{Linear Weighting} no es capaz de obtner soluciones en las partes c\'oncavas del frente.

    % TODO: Que condicion cumple los pesos de Chebychev
    \item Chebychev Distance (CSP): Se establece un punto $z^*$ que domina a todo el frente de Pareto y se utiliza la distancia de Chebychev como funci\'on objetivo utilizando un vector de pesos $\lambda$
    \begin{align*}
        \min \quad \max_{i \in {1,...,m}} \lambda_i |f_i(x) - z^*_i|, x \in X 
    \end{align*}
    CSP dado los pesos indicados puede encontrar cualquier punto del frente de Pareto, no importa si es c\'oncavo o convexo.

\end{enumerate}

\subsection{Algoritmos Num\'ericos}

% TODO: Es esto verdad?
En principio todos los algoritmos de escalarizaci\'on se pueden resolver utilzando m\'etodos n\'umericos.\\

Adem\'as, existen m\'etodos n\'umericos que intentan resolver el problema haciendo cumplir las condiciones de Karush-Kuhn-Tucker.

La idea va de encontrar al menos una soluci\'on al sistema de ecuaciones creado por tratar de resolver KKT. Luego utilizando m\'etodos de continuaci\'on y homotop\'ia para añadir al conjunto soluci\'on soluciones cercanas a estas. Este algoritmo require que las soluciones satisfagan las condiciones de convexidad local y diferenciabilidad.\\

Tambien existen otros metodos para la b\'usqueda de m\'inimos globales como \textit{T\'ecnicas de Subdivisi\'on}, \textit{Optimizaci\'on Global Bayesiana} y \textit{Optimizaci\'on de Lipschitz}. Estas requiren de que el espacio de decisi\'on tenga una baja dimensionalidad.

% TODO: Biibliografia de estos
Aun m\'as, tambi\'en se investiga activamente m\'etodos n\'umericos que no dependan de derivadas para su resoluci\'on.

\subsection{Algoritmos Gen\'eticos Multiobjetivos (MOEA)}

Los algorimtos gen\'eticos tuvieron sus inicios en la decada del 60 y fueron usados principalmente en resolucion de problemas num\'ericos combinatorios y no convexos. Utilizan paradigmas extra\'idos de la naturaleza, tal como seleccion natural, mutaci\'on y recombinaci\'on para mover una poblaci\'on (o conjunto de vectores de decisi\'on) hacian una soluci\'on \'optima.

Los algoritmos gen\'eticos multiobjetivos generalizan esta idea, y son diseñados para en cada iteraci\'on acercarse cada vez m\'as al frente de Pareto. Como en este caso no existe soluci\'on \'unica la manera de selccionar los individuos cambia fundamentalmente.

Dentro de los MOEA existen tres paradigmas principales:
\begin{enumerate}
    \item \textbf{MOEA basados en el frente de Pareto}: Se identifican porque dividen el proceso de selecci\'on en dos etapas. Una primera donde seleccionan los invidiuos segu\'un su ind\'ice de dominaci\'on, donde las soluciones que pertenecen al frente de Pareto tienen ind\'ice cero. Luego se realiza una segunda selecci\'on entre los ya seleccionados utilizando una segunda estrategia de puntuaci\'on. NSGA-II y SPEA2 son algoritmos de este tipo.

    \item \textbf{Basados en indicador}: Estos utilizan un indicador para calcular cuan cercano es el conjunto actual al frente de Pareto (unario), o cuanto mejora el nuevo conjunto de soluciones respecto a la iteraci\'on anterior (binario). Ejemplo de este es SMS-EMOEA que suele converger al frente de Pareto con soluciones igualmente distribuidas.

    \item \textbf{Basados en descomposici\'on}: La idea principal consiste en descomponer el problema en pequeños subproblemas cada una correspondiente a una secci\'on del frente de Pareto. Por cada sub-problema se resuelve utilizando escalarizaci\'on con dierente paramtrizaci\'on. El m\'etodo de escalarizaci\'on m\'as usado en estos casos suele ser CSP debido a ser capaz de obtener cualquier punto del frente de Pareto. Ejemplo de este paradigma son MOEA/D y NSGA-III.
\end{enumerate}

% TODO: Que es hypervolumen??

\section{Optimizaci\'on Multiobjetivo y AutoML}

Aunque es cada vez un feature de m\'as demanda en la actualidad, no se conoce ning\'un sistema de AutoML que tenga implementado multiobjetivo, excepto por TPOT que optimiza sus pipelines mutuamente para `accuracy` y tiempo de entranamiento. No obstante no presenta flexibiliad sobre que m\'etricas optimizar.\\


