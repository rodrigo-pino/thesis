% TODO: Bibliografia
\chapter{Estado del Arte}\label{chapter:state-of-the-art}

\section{Optimizaci\'on Multiobjetivo}

Optimizaci\'on Multiobjetivo es la rama de la Ciencia y la Matem\'atica que se dedica a optimizar para varias funciones objetivos sim\'ultaneamente.\\

\textit{Optimizaci\'on Multiobjetivo}: Dado $m$ funciones objetivos: $f_1: \mathcal{X} \rightarrow \mathbb{R}, ..., f_m: \mathcal{X} \rightarrow \mathbb{R}$ que dado un vector en el espacio de decisi\'on $\mathcal{X}$ es transformado en un valor de $\mathbb{R}$. M\'as formalmente:
\begin{align*}
    \text{MOP: }\min f_1(x), ..., f_m(x), x \in \mathcal{X}
\end{align*}

En presencia de varias funciones objetivos ya no es posible hablar de una soluci\'on \'unica mejor que el resto.  
Como las solucions se comparan respecto a dos o m\'as aspectos, se habla de dominaci\'on.\\

\textit{Pareto Dominaci\'on}: Dados vectors en el el espacio objetivo, $x \in \mathbb{R}^m$ y $y \in \mathbb{R}^m$ , se dice que $x$ Pareto domina a $y$ (i.e. $x \prec y$), si y solo si:
\begin{align*}
    \forall i \in \{1, ..., m\}: x_i \leq y_i \text{ and } \exists j \in \{1, ..., m\}: x_j < y_j 
\end{align*}


% Aqui tirar una foto prodr\'ia ser muy \'util
Existe el caso en el conjunto de soluciones que existan soluciones que no sean dominadas por ninguna otra soluci\'on. Estas soluciones vienen siendo el conjunto solucion del problema de optimziaci\'on multiobjetivo.\\

% Es usual que estos criterios de decisi\'on entren en conflictos entre s\'i y no sea posible encontrar una soluci\'on que satisfaga ambas m\'etricas, en cambio, se encuentran soluciones que si se intentan optimizar en algun aspecto, inevitablemente empeoran en otros. Estas soluciones son las que se conocen como soluciones del frente de Pareto. Con el concepto de Pareto viene el concepto de Pareto dominaci\'on.\\

% TODO: Improve definition
\textit{Frente de Pareto}: Todos los vectores $x$ del espacio objetivo $\mathcal{Y}$ tal que no exista $y \prec x$.
\begin{align*}
    \mathcal{P} = \{x| x, y \in \mathcal{Y}, \neg \exists y \prec x \} 
\end{align*}

Encontrar puntos del frente de Pareto no es tarea d\'ificil, puede ser tan trivial con encontrar el m\'inimo de cada funci\'on objetivo. La complejidad yace en encontrar un conjunto soluci\'on que sea lo m\'as aproximado posible al frente de Pareto.

Se debe notar que el termino Optimizaci\'on Multiobjetivo se utiliza cuando se optimza simult\'aneamente para dos o tres funciones objetivos. Para un cantidad de m\'etricas mayor se le conoce coloquialmente en la literatura como Optimizaci\'on para Muchos Objetivos o \textit{many-objective optimization} en ingl\'es. Se hace enf\'asis en esta  diferenciaci\'on este pues al aumentar le n\'umero de m\'etricas a optimizar:
\begin{enumerate}
    \item ya no es posible visualizar el frente obtenido;
    \item la computaci\'on de indicadores o de selecci\'on para muchos algoritmos se convierte en problemas NP-duros;
    \item existe un r\'apido crecimiento de puntos no dominados, mientras mayor n\'umero de objetivos, la probabilidad de que un punto sea no dominado en un set con distribuci\'on normal tiende exponencialmente a 1.
\end{enumerate}


\subsection{T\'ecnicas de Escalarizaci\'on (Scalarization)}
Los manera cl\'asica de resolver MOP es utilizando t\'ecinas de Escalarizaci\'on que es cuando de alguna manera u otra se combinan todas las funciones objetivos en una sola. Existen diferentes t\'ecinas de combinar estas funciones:
\begin{enumerate}
    \item Linear Weighting: Todas las funciones objetivos se combinan en una sola, y a cada una se le asigna un peso. Modificando estos pesos se obtiene una soluci\'on distinta del frente de Pareto.\\
    \begin{align*}
        \min \sum w_i f_i(x), \space x \in X
    \end{align*}
    Esta enfoque presenta el problema de que cuando el frente de Pareto no es convexo (tiene alguna porci\'on c\'oncava) no es posible obtener soluciones en esta zona, no importa como se modifiquen los pesos $w_i$.

    \item $\epsilon$-constrain: Se selecciona una funci\'on objetivo como principal y las dem\'as se establecen como restricciones de esta, y tienen que ser menor que sierto $\epsilon_i$ con $1 \leq n - 1$ por cada funci\'on objetivo.
    \begin{align*}
            \min  f_1(x), \space x \in X  & \text{, sujeto a:}   \\
            g_i(x) \leq \epsilon_i & \quad  \forall i, 0 \leq i \leq n - 1
    \end{align*}
    Utilizar esta t\'ecninca tiene dos dificultades principales, primero la obtenci\'on de los $\epsilon_i$, para una adecuada selecci\'on requiren conocimiento previo del frente de Pareto y al igual que \textit{Linear Weighting} no es capaz de detectar soluciones en las partes c\'oncavas del frente.

    % TODO: Que condicion cumple los pesos de Chebychev
\item Chebychev Distance (CSP): Se establece un punto de referencia $z^*$ y se utiliza la distancia de Chebychev de los vectores objetivos hacia este como funci\'on objetivo utilizando un vector de pesos $\lambda \in \mathbb{R}^m_{\prec 0}$, donde $\mathbb{R}^m_{\prec 0} = \{x | x \in \mathbb{R}^m, x \prec 0 \}$. 
    \begin{align*}
        \min \quad \max_{i \in {1,...,m}} \lambda_i |f_i(x) - z^*_i|, x \in X 
    \end{align*}
    CSP dado los pesos indicados puede encontrar cualquier punto del frente de Pareto, no importa si es c\'oncavo o convexo.

\end{enumerate}

\subsection{Algoritmos Num\'ericos}

% TODO: Es esto verdad?
En principio todos los algoritmos de escalarizaci\'on se pueden resolver utilzando m\'etodos n\'umericos.\\

Adem\'as, existen m\'etodos n\'umericos que intentan resolver el problema haciendo cumplir las condiciones de Karush-Kuhn-Tucker.

La idea va de encontrar al menos una soluci\'on al sistema de ecuaciones creado por tratar de resolver KKT. Luego utilizando m\'etodos de continuaci\'on y homotop\'ia para añadir al conjunto soluci\'on soluciones cercanas a estas. Este algoritmo require que las soluciones satisfagan las condiciones de convexidad local y diferenciabilidad.\\

Tambien existen otros metodos para la b\'usqueda de m\'inimos globales como \textit{T\'ecnicas de Subdivisi\'on}, \textit{Optimizaci\'on Global Bayesiana} y \textit{Optimizaci\'on de Lipschitz}. Estas requiren de que el espacio de decisi\'on tenga una baja dimensionalidad.

% TODO: Biibliografia de estos
Aun m\'as, tambi\'en se investiga activamente m\'etodos n\'umericos que no dependan de derivadas para su resoluci\'on.

\subsection{Algoritmos Gen\'eticos Multiobjetivos (MOEA)}

Los algorimtos gen\'eticos tuvieron sus inicios en la decada del 60 y fueron usados principalmente en resolucion de problemas num\'ericos combinatorios y no convexos. Utilizan paradigmas extra\'idos de la naturaleza, tal como seleccion natural, mutaci\'on y recombinaci\'on para mover una poblaci\'on (o conjunto de vectores de decisi\'on) hacian una soluci\'on \'optima.

Los algoritmos gen\'eticos multiobjetivos generalizan esta idea, y son diseñados para en cada iteraci\'on acercarse cada vez m\'as al frente de Pareto. Como en este caso no existe soluci\'on \'unica la manera de selccionar los individuos cambia fundamentalmente.

Dentro de los MOEA existen tres paradigmas principales:
\begin{enumerate}
    \item \textbf{MOEA basados en el frente de Pareto}: Se identifican porque dividen el proceso de selecci\'on en dos etapas. Una primera donde seleccionan los invidiuos segu\'un su ind\'ice de dominaci\'on, donde las soluciones que pertenecen al frente de Pareto tienen ind\'ice cero. Luego se realiza una segunda selecci\'on entre los ya seleccionados utilizando una segunda estrategia de puntuaci\'on. NSGA-II y SPEA2 son algoritmos de este tipo.

    \item \textbf{Basados en indicador}: Estos utilizan un indicador para calcular cuan cercano es el conjunto actual al frente de Pareto (unario), o cuanto mejora el nuevo conjunto de soluciones respecto a la iteraci\'on anterior (binario). Ejemplo de este es SMS-EMOEA que suele converger al frente de Pareto con soluciones igualmente distribuidas.

    \item \textbf{Basados en descomposici\'on}: La idea principal consiste en descomponer el problema en pequeños subproblemas cada una correspondiente a una secci\'on del frente de Pareto. Por cada sub-problema se resuelve utilizando escalarizaci\'on con dierente paramtrizaci\'on. El m\'etodo de escalarizaci\'on m\'as usado en estos casos suele ser CSP debido a ser capaz de obtener cualquier punto del frente de Pareto. Ejemplo de este paradigma son MOEA/D y NSGA-III.
\end{enumerate}

% TODO: Que es hypervolumen??

\section{Optimizaci\'on Multiobjetivo y AutoML}

Aunque es cada vez un feature de m\'as demanda en la actualidad, no se conoce ning\'un sistema de AutoML que tenga implementado multiobjetivo, excepto por TPOT que optimiza sus pipelines mutuamente para `accuracy` y tiempo de entranamiento. No obstante no presenta flexibiliad sobre que m\'etricas optimizar.\\


